{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5y/4Z32aC+ZPXyQ99uO8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mobeenahmedmehr/Mobeen_Ahmed/blob/main/RAG_project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing Required Libraries"
      ],
      "metadata": {
        "id": "hngDFEUNWwEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "id": "4LOVDBenV0Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64edc456-8cae-4e89-c664-c531b42f3287"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What it does:\n",
        "## This line installs two Python libraries:\n",
        "\n",
        "* **langchain-pinecone**: This library helps in working with Pinecone, a vector\n",
        "database used for storing and searching embeddings (numerical representations of text).\n",
        "\n",
        "* **langchain-google-genai**: This library provides tools to interact with Google's Generative AI models, like embeddings and chat models.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yCiWCJ8RW2ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting Up Pinecone"
      ],
      "metadata": {
        "id": "Jc_RcViWXnT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pinecone_api_key = userdata.get('PINECONEAPIKEY')\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "viQreMN9RC2-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "* **userdata.get('PINECONEAPIKEY')**: This retrieves your Pinecone API key from Google Colab's user data. You need to store your API key in Colab's secrets manager.\n",
        "\n",
        "* **Pinecone(api_key=pinecone_api_key)**: This initializes the Pinecone client using your API key, allowing you to interact with Pinecone's services."
      ],
      "metadata": {
        "id": "RrqKfOw1aYfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Creating a Pinecone Index**\n"
      ],
      "metadata": {
        "id": "sl7Yj9rPYhYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"simple-rag-project\"  # change if desired\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "Ql4MMXBYRKIY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "* **pc.create_index(...)**: This creates a new index in Pinecone. An index is like a database table where you store vectors (embeddings).\n",
        "\n",
        "* **name**: The name of the index.\n",
        "\n",
        "* **dimension=768**: The size of the vectors you'll store. Here, each vector has 768 dimensions.\n",
        "\n",
        "* **metric=\"cosine\"**: The similarity metric used to compare vectors. Cosine similarity is a common choice for text embeddings.\n",
        "\n",
        "* **spec=ServerlessSpec(...)**: This specifies that the index will be hosted on AWS in the us-east-1 region.\n",
        "\n",
        "* **index = pc.Index(index_name)**: This connects to the newly created index."
      ],
      "metadata": {
        "id": "zshVcZ7raolS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Setting Up Google Generative AI Embeddings"
      ],
      "metadata": {
        "id": "lcQDNJyjaqBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('Gemini_API_KEY')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "bu0WKgMSRVSJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**os.environ['GOOGLE_API_KEY']**: This sets the Google API key in the environment variables, which is required to use Google's Generative AI services.\n",
        "\n",
        "🌸**GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")**: This initializes the embeddings model, which converts text into vectors (embeddings)."
      ],
      "metadata": {
        "id": "tZCfavRIa8y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Creating an Embedding"
      ],
      "metadata": {
        "id": "V59KUlT1bH1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"We are building a RAG Text\")"
      ],
      "metadata": {
        "id": "CtcRvTMwTJ2u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K96aHHg1TOZz",
        "outputId": "aba354c5-24c9-4ef6-f08b-6462d3175d6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03732863813638687,\n",
              " -0.054927337914705276,\n",
              " -0.05723658204078674,\n",
              " -0.016060668975114822,\n",
              " 0.008930283598601818]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**embeddings.embed_query(...)**: This converts the text \"We are building a RAG Text\" into a vector (embedding).\n",
        "\n",
        "🌸**vector[:5]**: This shows the first 5 values of the vector. Vectors are just lists of numbers that represent the text in a way that computers can understand."
      ],
      "metadata": {
        "id": "ZFIHcKdTb4hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Setting Up Pinecone Vector Store"
      ],
      "metadata": {
        "id": "5Z-72K2-cLON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "ZZxw7wjsTTGI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**PineconeVectorStore(...)**: This sets up a vector store, which is a way to store and search vectors in Pinecone. It uses the index we created earlier and the embeddings model."
      ],
      "metadata": {
        "id": "PdwgDSyZcYmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Creating Documents"
      ],
      "metadata": {
        "id": "8t9zr2lecceT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]"
      ],
      "metadata": {
        "id": "avM9J0g5TZED"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dInPCm7sTe16",
        "outputId": "7d15dc9c-0711-4568-f99d-75c009353a52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**Document(...)**: This creates a document object that contains text (page_content) and metadata (like the source of the text).\n",
        "\n",
        "🌸**documents**: This is a list of all the documents you want to store in the vector store."
      ],
      "metadata": {
        "id": "Yib-GCdEcqO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Adding Documents to the Vector Store"
      ],
      "metadata": {
        "id": "byM5Dd1NdEvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "uuid4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFizK-28TxQ0",
        "outputId": "81d24662-cdb7-499d-bce5-54f4527dbd38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UUID('85dd2a19-605d-43f7-bdf1-49dffd2d174d')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2jW9LGoUTIB",
        "outputId": "13b3f085-b918-499f-817a-d194a322cdbc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['57919327-463b-4ff5-93dc-26f9f0bfa558',\n",
              " 'af4fc029-b8f8-4a02-a4d3-d70711d595da',\n",
              " '8276be3a-72f0-490f-9663-448e0d950fa4',\n",
              " '20cced97-28d7-492a-ba56-852770ea9c6e',\n",
              " 'f3073b3a-c5be-4a78-b61e-25e7ba6d62d8',\n",
              " 'a31f5233-56f0-4627-9fda-d93b15b7b336',\n",
              " 'db4c2044-b46c-4ce9-a833-84bb37aa0c04',\n",
              " '51cc108b-5a0e-4e8c-a0e5-b05ecd4ce11e',\n",
              " '8794c5b2-0d40-47c8-8c07-8b0baf271f78',\n",
              " '9141da1c-781d-4d38-b11a-163735c55081']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**uuid4()**: This generates a unique identifier (UUID) for each document.\n",
        "\n",
        "🌸**vector_store.add_documents(...)**: This adds the documents to the vector store, associating each document with a unique ID."
      ],
      "metadata": {
        "id": "WdY-wEeqdT-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Searching for Similar Documents"
      ],
      "metadata": {
        "id": "4413GHOqdfFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "nvZ6KMM2Tx3y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"How to bake Cookies?\",\n",
        "\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "BlNk4N3vT60k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**similarity_search(...)**: This searches for documents that are similar to the query \"LangChain provides abstractions to make working with LLMs easy\".\n",
        "\n",
        "🌸**k=2**: It returns the top 2 most similar documents.\n",
        "\n",
        "🌸**filter={\"source\": \"tweet\"}**: It only considers documents that have a \"source\" metadata value of \"tweet\".\n",
        "\n",
        "The **results** are printed, showing the content and metadata of the similar documents"
      ],
      "metadata": {
        "id": "gWzqZ5jQdr2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Using a Chat Model for Answering Questions"
      ],
      "metadata": {
        "id": "c-ZeTuhndubN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "wq6dlRa-UDgW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**ChatGoogleGenerativeAI(...)**: This initializes a chat model from Google's Generative AI. It will be used to generate answers based on the user's query and the documents found in the vector store."
      ],
      "metadata": {
        "id": "5RxacjAweHgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Answering User Queries"
      ],
      "metadata": {
        "id": "wa8DtYB9eLdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_to_user(query: str):\n",
        "\n",
        "  # Vector Search\n",
        "  vector_results = vector_store.similarity_search(query, k=2)\n",
        "  print(len(vector_results))\n",
        "\n",
        "  # TODO: Pass to Model Vector Results + User Query\n",
        "  final_answer = llm.invoke(f\"ANSWER THIS USER QUERY: {query}, Here are some references to answer {vector_results}\")\n",
        "\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "ftgigVy_Uhrx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸**vector_store.similarity_search(query, k=2)**: This searches for the top 2 documents similar to the user's query.\n",
        "\n",
        "🌸**llm.invoke(...)**: This sends the user's query and the similar documents to the chat model, asking it to generate an answer based on the provided information."
      ],
      "metadata": {
        "id": "U4VPm-z8ecKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Example Usage"
      ],
      "metadata": {
        "id": "DhCi4kvQeepR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_to_user(\"LangChain provides abstractions to make working with LLMs easy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBP8YKQ3UlZo",
        "outputId": "c2413c70-1cfc-4473-a1ed-eacfdd5bb559"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "20hcrlVXUpvO",
        "outputId": "78175afc-4b63-4e85-bef1-6d3fe5a51c1f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the provided documents, LangChain is a framework used for building applications, and one user is excited about a new project built with it.  The documents don't directly support the claim that LangChain provides abstractions to make working with LLMs easy, although that is a common understanding of LangChain's functionality.  The first document mentions LangGraph, a different framework, suggesting there are multiple options for building applications involving LLMs.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What it does:\n",
        "\n",
        "🌸This calls the **answer_to_user** function with a specific query and prints the generated answer."
      ],
      "metadata": {
        "id": "uLvbpVJlenMM"
      }
    }
  ]
}